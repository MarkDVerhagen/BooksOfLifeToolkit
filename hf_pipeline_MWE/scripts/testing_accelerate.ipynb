{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vs3041/.conda/envs/hf_environment/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model, LoraConfig, AutoPeftModelForSequenceClassification\n",
    "import argparse\n",
    "import pandas as pd \n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jdqi8t8x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /scratch/gpfs/vs3041/prefer_prepare/hf_pipeline_MWE/scripts/wandb/offline-run-20240723_081519-jdqi8t8x<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20240723_081519-jdqi8t8x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jdqi8t8x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setting wandb to offline\n",
    "wandb.init(mode=\"offline\")   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_salganik_data(data):\n",
    "\n",
    "    ## Taking csv and turning it into HF format\n",
    "\n",
    "    # HF needs a specific form for the dataset\n",
    "    data[\"text\"] = data[\"input\"]\n",
    "    data[\"labels\"] = data[\"output\"]\n",
    "\n",
    "    # keeping only inputs and outputs\n",
    "    data = data[[\"text\", \"labels\"]]\n",
    "\n",
    "    # making sure outputs are 0s and 1s (will need to make this programatic)\n",
    "    data[\"labels\"] = (data[\"labels\"] == \"kid: 1\").astype(int)  \n",
    "\n",
    "    # converting to HF format\n",
    "    data = Dataset.from_pandas(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_unique_id(filename):\n",
    "\n",
    "    # extract from books of life \n",
    "    if \"bol/\" in filename:\n",
    "        start = filename.index('bol/') + len('bol/')\n",
    "        end = filename.index('.txt')\n",
    "\n",
    "    elif \"outcome/\" in filename:\n",
    "        start = filename.index('outcome/') + len('outcome/')\n",
    "        end = filename.index('.txt')\n",
    "    else: \n",
    "        ValueError(\"Can't find unique id from filename\")\n",
    "    \n",
    "    return filename[start:end]\n",
    "\n",
    "def format_BOL_data(path_to_training_data):\n",
    "    \n",
    "    # Step 1: reading in books of life\n",
    "\n",
    "    # the books of life are contained in multiple .txt files \n",
    "    BOL_txt_files = glob.glob(path_to_training_data + \"bol/\" + '*.txt')\n",
    "\n",
    "    books_of_life = []\n",
    "    unique_ids = []\n",
    "    data = []\n",
    "\n",
    "    for txt_file in BOL_txt_files:\n",
    "\n",
    "        #reading books of life\n",
    "        with open(txt_file, 'r') as infile:\n",
    "            book_of_life = infile.read().strip()\n",
    "            unique_id = extract_unique_id(txt_file)     # extract unique_id from filename\n",
    "\n",
    "        \n",
    "        # reading outcomes\n",
    "        outcome_txt_file = path_to_training_data + \"outcome/\" + unique_id + '.txt'\n",
    "        with open(outcome_txt_file, 'r') as infile:\n",
    "            outcome = infile.read().strip()\n",
    "\n",
    "        data_for_unique_id = {\n",
    "            \"text\": book_of_life,\n",
    "            \"unique_id\": unique_id,\n",
    "            \"labels\": int(outcome)\n",
    "        }\n",
    "\n",
    "        data.append(data_for_unique_id)\n",
    "\n",
    "    # added with_format(\"torch\")\n",
    "    data = Dataset.from_list(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "def tokenize_and_prepare(data):\n",
    "\n",
    "    # Tokenizer for HF models\n",
    "\n",
    "    return tokenizer(data[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# model arguments from the command line\n",
    "\n",
    "\n",
    "\n",
    "# # model options\n",
    "#export model_name=\"llama3-8b\"\n",
    "#export dataset=\"bol-temp-1\"\n",
    "#export fine_tune_method=\"lora\"\n",
    "#export GPU_util=\"single\"\n",
    "#export params=\"default\"\n",
    "#export training_folds=\"0-0\"\n",
    "#export test_folds=\"0-0\"\n",
    "\n",
    "model_name = \"llama3-8b\"\n",
    "dataset = \"bol-temp-1\"\n",
    "fine_tune_method = \"lora\"\n",
    "GPU_util = \"single\"\n",
    "params = \"default\"\n",
    "training_folds = \"0-0\"\n",
    "first_training_fold, last_training_fold = map(int, training_folds.split(\"-\"))\n",
    "\n",
    "project_directory = \"/scratch/gpfs/vs3041/prefer_prepare/hf_pipeline_MWE/\"\n",
    "\n",
    "fine_tuned_model_name = \"-\".join([model_name, dataset, fine_tune_method, GPU_util, params, \"folds\", training_folds])\n",
    "output_directory = project_directory + \"fine_tuned_models/\" + fine_tuned_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bol-temp-1\n",
      "samples = 13642\n"
     ]
    }
   ],
   "source": [
    "#### SPECIFIYING MODEL\n",
    "\n",
    "if model_name == \"llama3-8b\":\n",
    "    model_to_read = project_directory + \"hf_models/\" + \"Meta-Llama-3-8B/\"\n",
    "else: \n",
    "    raise Exception(\"Either the model is the wrong place, or we haven't downloaded it yet :(\")\n",
    "\n",
    "\n",
    "#### SPECIFIYING DATA\n",
    "\n",
    "if dataset == \"salganik\":\n",
    "    # reading training data \n",
    "    data_to_read = project_directory + \"data/salganik_data.csv\"\n",
    "    train_dataset = pd.read_csv(data_to_read)\n",
    "\n",
    "    # subsetting data to specified training folds \n",
    "    train_dataset = train_dataset[train_dataset['fold'].between(first_training_fold, last_training_fold)]\n",
    "\n",
    "    # formatting the salganik data to get it into a format readable by the LLM\n",
    "    train_dataset = format_salganik_data(train_dataset)    \n",
    "\n",
    "    print(\"using Salganik data\")\n",
    "\n",
    "elif dataset == \"bol-temp-1\" or dataset == \"bol-temp-2\":\n",
    "    data_to_read =  \"/scratch/gpfs/vs3041/prefer_prepare/synth/data/e2e/test_template1/train/\"\n",
    "\n",
    "    train_dataset = format_BOL_data(data_to_read)\n",
    "\n",
    "    print(\"Using \" + dataset)\n",
    "\n",
    "    print(\"samples = \" + str(len(train_dataset)))\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Dataset not recognised\")\n",
    "\n",
    "#### SPECIFIYING FINE-TUNING METHOD\n",
    "\n",
    "if fine_tune_method == \"full\":\n",
    "    pass\n",
    "elif fine_tune_method == \"lora\":\n",
    "    pass\n",
    "else: \n",
    "    raise Exception(\"Haven't written code to support other fine-tune methods yet\")\n",
    "\n",
    "\n",
    "#### SPECIFIYING GPU UTILIZATION\n",
    "\n",
    "if GPU_util == \"single\":\n",
    "    pass\n",
    "else: \n",
    "    raise Exception(\"Haven't written code to support distributed GPU utilization yet\")\n",
    "\n",
    "\n",
    "#### ADDING CUSTOM HYPERPARAMETERS\n",
    "\n",
    "if params == \"default\":\n",
    "    pass\n",
    "else: \n",
    "    raise Exception(\"Haven't written code to change parameters yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:07<00:00, 16.95s/it]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /scratch/gpfs/vs3041/prefer_prepare/hf_pipeline_MWE/hf_models/Meta-Llama-3-8B/ and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# might help with CUDA issues: https://blog.gopenai.com/how-to-resolve-runtimeerror-cuda-out-of-memory-d48995452a0\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# setting up model architecture and initializing tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_to_read, num_labels=2, device_map = \"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_to_read)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model\n",
      "trainable params: 13,639,680 || all params: 7,518,572,544 || trainable%: 0.1814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13642/13642 [00:04<00:00, 2742.44 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# tokenizing the training data \u001b[39;00m\n\u001b[1;32m     39\u001b[0m tokenized_train_data \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_and_prepare, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 42\u001b[0m trainer \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39mprepare(\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_train_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m)\u001b[49m)\n\u001b[1;32m     50\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     52\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/hf_environment/lib/python3.12/site-packages/transformers/trainer.py:529\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    528\u001b[0m ):\n\u001b[0;32m--> 529\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/.conda/envs/hf_environment/lib/python3.12/site-packages/transformers/trainer.py:776\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 776\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/hf_environment/lib/python3.12/site-packages/accelerate/big_modeling.py:455\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 455\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# initializing accelerate\n",
    "accelerator = Accelerator()\n",
    "\n",
    "\n",
    "if fine_tune_method == \"lora\":\n",
    "    peft_config = LoraConfig(task_type=\"SEQ_CLS\", \n",
    "                         inference_mode=False, \n",
    "                         r=32, \n",
    "                         lora_alpha=16, \n",
    "                         lora_dropout=0.1)\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "    print('PEFT Model')\n",
    "    peft_model.print_trainable_parameters()\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "# model hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    logging_steps=20,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    gradient_accumulation_steps=8, # improves memory utilization\n",
    "    # weight_decay=0.01\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    save_strategy = \"no\"  # will save model manually usuing \n",
    ")\n",
    "\n",
    "# input padding options \n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# tokenizing the training data \n",
    "tokenized_train_data = train_dataset.map(tokenize_and_prepare, batched=True)\n",
    "\n",
    "\n",
    "trainer = accelerator.prepare(Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Fine-tuning time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1505ddf7d670>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
